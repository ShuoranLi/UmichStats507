{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats507 Homework10, Winter 2019\n",
    "### Shuoran Li\n",
    "#### shuoranl@umich.edu\n",
    "\n",
    "I did not discuss problems with anyone else in the class on this homework.\n",
    "\n",
    "Problem 1 took me 0.5 hour; Problem 2 took me 3 hours; Problem 3 took me 3 hours; Problem 4 took me 6 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Warmup: Constructing a 3-tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "tflogo = tf.constant([[[0,0,1],[0,0,1],[0,0,1],[1,1,1]],\n",
    "                      [[0,0,0],[0,0,0],[0,0,1],[0,0,0]],\n",
    "                      [[0,0,0],[0,0,0],[0,1,1],[0,0,0]],\n",
    "                      [[0,0,0],[0,0,0],[0,0,1],[0,0,0]],\n",
    "                      [[0,0,0],[0,0,0],[0,0,1],[0,0,0]]],dtype = tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 1]\n",
      "  [0 0 1]\n",
      "  [0 0 1]\n",
      "  [1 1 1]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 1]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 1 1]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 1]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 1]\n",
      "  [0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    print(tflogo.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Building and training simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "#### Logistic regression with a negative log-likelihood loss. In this model, which we discussed briefly in class, the binary variable Y is distributed as a Bernoulli random variable with success parameter σ(W T X + b), where σ(z) = (1 + exp(−z))−1 is the logistic function, and X ∈ R6 is the predictor random variable, and W ∈ R6, b ∈ R are the model parameters. Derive the log-likelihood of Y , and write the TensorFlow code that represents the negative log-likelihood loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,6])\n",
    "W = tf.Variable(tf.zeros([6,1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = tf.sigmoid(tf.matmul(x,W)+b)\n",
    "\n",
    "ytrue = tf.placeholder(tf.float32,[None,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood of Y is $\\sum^N_1Y_ilog(\\frac{e^{W^TX_i+b}}{1+e^{W^TX_i+b}})+(1-Y_i)log(\\frac{1}{1+e^{W^TX_i+b}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_loglik = -tf.reduce_sum(ytrue*tf.log(y)+(1-ytrue)*tf.log(1-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "#### Estimating parameters in logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training data\n",
    "xtrain = np.load('logistic_xtrain.npy')\n",
    "ytrain = np.load('logistic_ytrain.npy')\n",
    "\n",
    "learn_rate = 0.0005 # Assign learning rate\n",
    "num_steps = 5000\n",
    "## write train-step using gradient descent\n",
    "train_step = tf.train.GradientDescentOptimizer(learn_rate).minimize(neg_loglik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "for _ in range(num_steps):\n",
    "    sess.run(train_step, feed_dict = {x:xtrain, ytrue:ytrain})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the estimation of W is \n",
      "[[0.97806984]\n",
      " [1.2326719 ]\n",
      " [1.4981933 ]\n",
      " [3.0152678 ]\n",
      " [4.643959  ]\n",
      " [7.5283117 ]]\n",
      "the estimation of b is \n",
      "[-0.9549102]\n"
     ]
    }
   ],
   "source": [
    "### the parameter W\n",
    "W_est = sess.run(W)\n",
    "b_est = sess.run(b)\n",
    "print('the estimation of W is \\n'+str(W_est)+\n",
    "     '\\nthe estimation of b is \\n'+str(b_est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "#### Evaluating logistic regression on test data. Load the test data. What is the negative log-likelihood of your model on this test data? That is, what is the negative log-likelihood when you use your estimated parameters with the previously unseen test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy on test set is 0.858\n",
      "the negative log-likelihood is 163.01442\n"
     ]
    }
   ],
   "source": [
    "xtest = np.load('logistic_xtest.npy')\n",
    "ytest = np.load('logistic_ytest.npy')\n",
    "#check accuracy\n",
    "pred_y = tf.cast(tf.greater(y,0.5),tf.float32)\n",
    "correct_prediction = tf.equal(pred_y,ytrue)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# get statistics we are interested in\n",
    "accuracy_test = sess.run(accuracy, feed_dict= {x:xtest, ytrue:ytest})\n",
    "neg_loglik_test = sess.run(neg_loglik,feed_dict = {x:xtest, ytrue:ytest})\n",
    "print('the accuracy on test set is '+str(accuracy_test)+\n",
    "     '\\nthe negative log-likelihood is '+str(neg_loglik_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4\n",
    "#### Evaluating the estimated logistic parameters.Write TensorFlow expressions to compute the squared error between your estimated parameters and their true values. Evaluate the error in recovering W and b sepa- rately. What are the squared errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_true = tf.constant([[1],[1],[2],[3],[5],[8]], dtype = tf.float32)\n",
    "b_true = tf.constant(-1,dtype = tf.float32)\n",
    "\n",
    "W_squareError = tf.reduce_sum(tf.square(W_est - W_true))\n",
    "b_squareError = tf.reduce_sum(tf.square(b_est - b_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The squared error of W is 0.6559152\n",
      "The squared error of b is 0.0020330884\n"
     ]
    }
   ],
   "source": [
    "print('The squared error of W is '+str(W_squareError.eval())+\n",
    "     '\\nThe squared error of b is '+str(b_squareError.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5\n",
    "#### Please make the variables from the above problems available in a dictionary called results_logistic. The dictionary should have keys ’W’, ’Wsqerr’, ’b’, ’bsqerr’, ’log_lik_test’,withrespectivevaluessess.run(x) where x ranges over the corresponding quantities. For example, if my squared error for W is stored in a TF variable called W_squared_error, then the key ’Wsqerr’ should have value sess.run(W_squared_error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_logistic = {'W':W_est, 'Wsqerr':sess.run(W_squareError),\n",
    "                   'b':b_est, 'bsqerr':sess.run(b_squareError),\n",
    "                   'log_lik_test':neg_loglik_test}\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W': array([[0.97806984],\n",
      "       [1.2326719 ],\n",
      "       [1.4981933 ],\n",
      "       [3.0152678 ],\n",
      "       [4.643959  ],\n",
      "       [7.5283117 ]], dtype=float32), 'Wsqerr': 0.6559152, 'b': array([-0.9549102], dtype=float32), 'bsqerr': 0.0020330884, 'log_lik_test': 163.01442}\n"
     ]
    }
   ],
   "source": [
    "print(results_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6\n",
    "#### Classification of normally distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "normal_xtest = np.load(\"normal_xtest.npy\")\n",
    "normal_xtrain = np.load(\"normal_xtrain.npy\")\n",
    "normal_ytrain = np.load(\"normal_ytrain.npy\")\n",
    "normal_ytest = np.load(\"normal_ytest.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "mu = tf.Variable(tf.zeros(3),dtype = tf.float32)\n",
    "sigma2 = tf.Variable(tf.ones(3), dtype = tf.float32)\n",
    "x_normal = tf.placeholder(tf.float32,[None,1])\n",
    "ytrue_normal = tf.placeholder(tf.float32,[None,3])\n",
    "# define predited y. i.e. the probability of \n",
    "y_normal = (2*math.pi*sigma2)**(-0.5)*tf.exp(-(x_normal-mu)**2/(2*sigma2))\n",
    "\n",
    "loss_normal = tf.reduce_mean(-tf.reduce_sum(ytrue_normal*tf.log(y_normal), reduction_indices=[1]))\n",
    "train_step_normal = tf.train.AdagradOptimizer(0.1).minimize(loss_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()\n",
    "for _ in range(5000):\n",
    "    sess.run(train_step_normal, feed_dict = {x_normal:normal_xtrain, ytrue_normal:normal_ytrain})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7\n",
    "#### Evaluating loss on test data. Load the test data. What is the cross-entropy of your model on this test data? That is, what is the cross-entropy when you use your estimated parameters with the previously unseen test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cross_entropy on test data is 1.372696\n"
     ]
    }
   ],
   "source": [
    "crossent_test = sess.run(loss_normal, feed_dict={x_normal:normal_xtest, ytrue_normal:normal_ytest} )\n",
    "print( 'the cross_entropy on test data is '+ str(crossent_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8\n",
    "#### Evaluating parameter estimation on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimation of mu is [-1.0076396  0.0044913  3.0048473]\n",
      "estimation of sigma2 is [0.5333826 1.0087496 1.5162542]\n"
     ]
    }
   ],
   "source": [
    "mu_est = sess.run(mu)\n",
    "sigma2_est = sess.run(sigma2)\n",
    "print('estimation of mu is '+str(mu_est)+\n",
    "     '\\nestimation of sigma2 is '+str(sigma2_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total squared error is 0.0015571839\n"
     ]
    }
   ],
   "source": [
    "mu_true = tf.constant([-1,0,3],dtype = tf.float32)\n",
    "sigma2_true = tf.constant([0.5,1,1.5],dtype = tf.float32)\n",
    "t_sqerr = tf.reduce_sum(tf.square(mu_true-mu_est))+tf.reduce_sum(tf.square(sigma2_true-sigma2_est))\n",
    "print('the total squared error is '+str(sess.run(t_sqerr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9\n",
    "#### Evaluating classification error on test data. Write and evaluate a TensorFlow expression that computes the classification error of your estimated model averaged over the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the classification error on test data is 0.2680000066757202\n"
     ]
    }
   ],
   "source": [
    "correct_prediction_normal = tf.equal(tf.argmax(y_normal,1),tf.argmax(ytrue_normal,1))\n",
    "accuracy_normal = tf.reduce_mean(tf.cast(correct_prediction_normal, tf.float32))\n",
    "class_error = 1-sess.run(accuracy_normal, feed_dict = {x_normal:normal_xtest, ytrue_normal:normal_ytest})\n",
    "print('the classification error on test data is '+str(class_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10\n",
    "#### Again, for ease of grading, define a dictionary called results_class, with keys ’mu’, ’sigma2’, ’crossent_test’, ’class_error’ with keys corresponding to the evaluation (again using sess.run) of your estimate of μ, σ2, the cross-entropy on the test set, and the classification error from the previous problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mu': array([-1.0076396,  0.0044913,  3.0048473], dtype=float32), 'sigma2': array([0.5333826, 1.0087496, 1.5162542], dtype=float32), 'crossent_test': 1.372696, 'class_error': 0.2680000066757202}\n"
     ]
    }
   ],
   "source": [
    "results_class = {'mu':mu_est,'sigma2':sigma2_est,'crossent_test':crossent_test,\n",
    "                'class_error':class_error}\n",
    "print(results_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4:Running Models on Google Cloud Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "#### Train the same classifier on the same training data, but this time, save the resulting trained model in a directory called normal_trained. You’ll want to use the tf.saved_model.simple_save function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.simple_save(sess,\n",
    "            export_dir='./normal_trained',\n",
    "            inputs={\"x\": x_normal},\n",
    "            outputs={\"z\": y_normal})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6\n",
    "#### Please in- clude a copy-paste of the command you ran to request this prediction as well as the resulting output. Which cluster does your model think x = 4 came from?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MODEL_NAME=\"stats507w19_hw10_normal\"\n",
    "INPUT_DATA_FILE=\"instance.hw10.json\"\n",
    "VERSION_NAME=\"v1\"\n",
    "gcloud ai-platform predict --model $MODEL_NAME  \\\n",
    "--version $VERSION_NAME \\\n",
    "--json-instances $INPUT_DATA_FILE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The resulting output is:\n",
    "Z\n",
    "3.37619e-11\n",
    "0.000145388\n",
    "0.23372"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So depending on our model, the class for x=4 should be the third class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
