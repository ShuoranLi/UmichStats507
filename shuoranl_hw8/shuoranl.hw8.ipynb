{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats507 Homework7, Winter 2019\n",
    "### Shuoran Li\n",
    "#### shuoranl@umich.edu\n",
    "\n",
    "I did not discuss problems with anyone else in the class on this homework.\n",
    "\n",
    "Problem 1 took me 1 hour; Problem 2 took me 4 hours; Problem 3 took me 4 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Statement: \n",
    "### <span style=\"color:red\">Based on my understanding, this notebook is supposed to show the command lines that I used to get answers. So I skip those sections that don't require me to write command lines in this file. But my work is complete, all the required .py files, .pdf files, .txt files are attached into zip file. If you have questions about details, you may need to refer to these files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Warmup: counting words with mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python mr_word_count.py simple.txt > simple_word_counts.txt\n",
    "\n",
    "#### Output:\n",
    "No configs found; falling back on auto-configuration\n",
    "No configs specified for inline runner\n",
    "Running step 1 of 1...\n",
    "Creating temp directory /var/folders/hh/m_mk_zzj2wbglc2fd1w2zcjm0000gp/T/mr_word_count.shuoranli.20190402.024617.522290\n",
    "job output is in /var/folders/hh/m_mk_zzj2wbglc2fd1w2zcjm0000gp/T/mr_word_count.shuoranli.20190402.024617.522290/output\n",
    "Streaming final output from /var/folders/hh/m_mk_zzj2wbglc2fd1w2zcjm0000gp/T/mr_word_count.shuoranli.20190402.024617.522290/output...\n",
    "Removing temp directory /var/folders/hh/m_mk_zzj2wbglc2fd1w2zcjm0000gp/T/mr_word_count.shuoranli.20190402.024617.522290..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1\n",
    "scp mr_word_count.py shuoranl@flux-hadoop-login.arc-ts.umich.edu:~/507"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2\n",
    "python mr_word_count.py hdfs:///var/stats507w19/darwin.txt -r hadoop > darwin_word_counts.txt\n",
    "\n",
    "#### Output:\n",
    "Using configs in /etc/mrjob.conf\n",
    "Looking for hadoop binary in $PATH...\n",
    "Found hadoop binary: /usr/bin/hadoop\n",
    "Using Hadoop version 2.7.3.2.6.3.0\n",
    "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
    "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
    "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
    "Creating temp directory /tmp/mr_word_count.shuoranl.20190402.021442.086626\n",
    "Copying local files to hdfs:///user/shuoranl/tmp/mrjob/mr_word_count.shuoranl.20190402.021442.086626/files/...\n",
    "Running step 1 of 1...\n",
    "  packageJobJar: [] [/usr/hdp/2.6.3.0-235/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.3.0-235.jar] /tmp/streamjob4697470157335889880.jar tmpDir=null\n",
    "  Connecting to ResourceManager at fladoop-rm01.arc-ts.umich.edu/10.164.5.157:8050\n",
    "  Connecting to Application History server at fladoop-rm01.arc-ts.umich.edu/10.164.5.157:10200\n",
    "  Connecting to ResourceManager at fladoop-rm01.arc-ts.umich.edu/10.164.5.157:8050\n",
    "  Connecting to Application History server at fladoop-rm01.arc-ts.umich.edu/10.164.5.157:10200\n",
    "  Created HDFS_DELEGATION_TOKEN token 133287 for shuoranl on 10.164.5.158:8020\n",
    "  Got dt for hdfs://fladoop-nn02.arc-ts.umich.edu:8020; Kind: HDFS_DELEGATION_TOKEN, Service: 10.164.5.158:8020, Ident: (HDFS_DELEGATION_TOKEN token 133287 for shuoranl)\n",
    "  Total input paths to process : 1\n",
    "  number of splits:2\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3\n",
    "scp shuoranl@flux-hadoop-login.arc-ts.umich.edu:~/507/darwin_word_counts.txt ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Computing Sample Statistics with mrjob "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python mr_summary_stats.py populations_small.txt > summary_small.txt\n",
    "\n",
    "#### Output:\n",
    "No configs found; falling back on auto-configuration\n",
    "No configs specified for inline runner\n",
    "Running step 1 of 2...\n",
    "Creating temp directory /var/folders/hh/m_mk_zzj2wbglc2fd1w2zcjm0000gp/T/mr_summary_stats.shuoranli.20190402.024933.817508\n",
    "Running step 2 of 2...\n",
    "job output is in /var/folders/hh/m_mk_zzj2wbglc2fd1w2zcjm0000gp/T/mr_summary_stats.shuoranli.20190402.024933.817508/output\n",
    "Streaming final output from /var/folders/hh/m_mk_zzj2wbglc2fd1w2zcjm0000gp/T/mr_summary_stats.shuoranli.20190402.024933.817508/output...\n",
    "Removing temp directory /var/folders/hh/m_mk_zzj2wbglc2fd1w2zcjm0000gp/T/mr_summary_stats.shuoranli.20190402.024933.817508..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1\n",
    "scp mr_summary_stats.py shuoranl@flux-hadoop-login.arc-ts.umich.edu:~/507"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2\n",
    "python mr_summary_stats.py hdfs:///var/stats507w19/populations_large.txt -r hadoop > summary_large.txt\n",
    "\n",
    "#### Output:\n",
    "Using configs in /etc/mrjob.conf\n",
    "Looking for hadoop binary in $PATH...\n",
    "Found hadoop binary: /usr/bin/hadoop\n",
    "Using Hadoop version 2.7.3.2.6.3.0\n",
    "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
    "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
    "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
    "Creating temp directory /tmp/mr_summary_stats.shuoranl.20190402.022019.386708\n",
    "Copying local files to hdfs:///user/shuoranl/tmp/mrjob/mr_summary_stats.shuoranl.20190402.022019.386708/files/...\n",
    "Running step 1 of 2...\n",
    "  packageJobJar: [] [/usr/hdp/2.6.3.0-235/hadoop-mapreduce/hadoop-streaming-2.7.3.2.6.3.0-235.jar] /tmp/streamjob1199980159414525567.jar tmpDir=null\n",
    "  Connecting to ResourceManager at fladoop-rm01.arc-ts.umich.edu/10.164.5.157:8050\n",
    "  Connecting to Application History server at fladoop-rm01.arc-ts.umich.edu/10.164.5.157:10200\n",
    "  Connecting to ResourceManager at fladoop-rm01.arc-ts.umich.edu/10.164.5.157:8050\n",
    "  Connecting to Application History server at fladoop-rm01.arc-ts.umich.edu/10.164.5.157:10200\n",
    "  Created HDFS_DELEGATION_TOKEN token 133296 for shuoranl on 10.164.5.158:8020\n",
    "  Got dt for hdfs://fladoop-nn02.arc-ts.umich.edu:8020; Kind: HDFS_DELEGATION_TOKEN, Service: 10.164.5.158:8020, Ident: (HDFS_DELEGATION_TOKEN token 133296 for shuoranl)\n",
    "  Total input paths to process : 1\n",
    "  Adding a new node: /default-rack/10.164.1.140:1019\n",
    "  Adding a new node: /default-rack/10.164.1.143:1019\n",
    "  Adding a new node: /default-rack/10.164.1.144:1019\n",
    "  Adding a new node: /default-rack/10.164.1.141:1019\n",
    "  Adding a new node: /default-rack/10.164.1.142:1019\n",
    "  Adding a new node: /default-rack/10.164.1.145:1019\n",
    "  number of splits:2\n",
    "  Submitting tokens for job: job_1547074859606_10758\n",
    "  \n",
    "  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3\n",
    "scp shuoranl@flux-hadoop-login.arc-ts.umich.edu:~/507/summary_large.txt ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Graph Processing: Counting Triangles with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1\n",
    "scp ps_fof.py shuoranl@flux-hadoop-login.arc-ts.umich.edu:~/507"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2\n",
    "spark-submit --master yarn --queue stats507w19 ps_fof.py hdfs:/var/stats507w19/fof/friends.simple trianglecount\n",
    "\n",
    "#### Output:\n",
    "SPARK_MAJOR_VERSION is set to 2, using Spark2\n",
    "19/04/01 20:36:34 INFO SparkContext: Running Spark version 2.2.0.2.6.3.0-235\n",
    "19/04/01 20:36:34 INFO SparkContext: Submitted application: TriangleCounting\n",
    "19/04/01 20:36:34 INFO SecurityManager: Changing view acls to: shuoranl\n",
    "19/04/01 20:36:34 INFO SecurityManager: Changing modify acls to: shuoranl\n",
    "19/04/01 20:36:34 INFO SecurityManager: Changing view acls groups to: \n",
    "19/04/01 20:36:34 INFO SecurityManager: Changing modify acls groups to: \n",
    "19/04/01 20:36:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(shuoranl); groups with view permissions: Set(); users  with modify permissions: Set(shuoranl); groups with modify permissions: Set()\n",
    "19/04/01 20:36:35 INFO Utils: Successfully started service 'sparkDriver' on port 40365.\n",
    "19/04/01 20:36:35 INFO SparkEnv: Registering MapOutputTracker\n",
    "19/04/01 20:36:35 INFO SparkEnv: Registering BlockManagerMaster\n",
    "19/04/01 20:36:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
    "19/04/01 20:36:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
    "19/04/01 20:36:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-416d9d4e-bb78-4460-b01e-b265dd2d7a2a\n",
    "19/04/01 20:36:35 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
    "19/04/01 20:36:35 INFO SparkEnv: Registering OutputCommitCoordinator\n",
    "19/04/01 20:36:35 INFO log: Logging initialized @3605ms\n",
    "19/04/01 20:36:35 INFO Server: jetty-9.3.z-SNAPSHOT\n",
    "19/04/01 20:36:35 INFO Server: Started @3693ms\n",
    "19/04/01 20:36:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "19/04/01 20:36:35 INFO AbstractConnector: Started ServerConnector@2758c0f{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n",
    "19/04/01 20:36:35 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3\n",
    "hdfs dfs -cat trianglecount/* > small_triangle_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step4\n",
    "scp shuoranl@flux-hadoop-login.arc-ts.umich.edu:~/507/small_triangle_list.txt ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1\n",
    "spark-submit --master yarn --queue stats507w19 ps_fof.py hdfs:/var/stats507w19/fof/friends1000 trianglecount2\n",
    "\n",
    "#### Output:\n",
    "SPARK_MAJOR_VERSION is set to 2, using Spark2\n",
    "19/04/01 20:51:54 INFO SparkContext: Running Spark version 2.2.0.2.6.3.0-235\n",
    "19/04/01 20:51:54 INFO SparkContext: Submitted application: TriangleCounting\n",
    "19/04/01 20:51:54 INFO SecurityManager: Changing view acls to: shuoranl\n",
    "19/04/01 20:51:54 INFO SecurityManager: Changing modify acls to: shuoranl\n",
    "19/04/01 20:51:54 INFO SecurityManager: Changing view acls groups to: \n",
    "19/04/01 20:51:54 INFO SecurityManager: Changing modify acls groups to: \n",
    "19/04/01 20:51:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(shuoranl); groups with view permissions: Set(); users  with modify permissions: Set(shuoranl); groups with modify permissions: Set()\n",
    "19/04/01 20:51:55 INFO Utils: Successfully started service 'sparkDriver' on port 42080.\n",
    "19/04/01 20:51:55 INFO SparkEnv: Registering MapOutputTracker\n",
    "19/04/01 20:51:55 INFO SparkEnv: Registering BlockManagerMaster\n",
    "19/04/01 20:51:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
    "19/04/01 20:51:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
    "19/04/01 20:51:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c107c97c-0b02-47a6-b8a3-a58e4d1f8383\n",
    "19/04/01 20:51:55 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
    "19/04/01 20:51:55 INFO SparkEnv: Registering OutputCommitCoordinator\n",
    "19/04/01 20:51:55 INFO log: Logging initialized @3111ms\n",
    "19/04/01 20:51:55 INFO Server: jetty-9.3.z-SNAPSHOT\n",
    "19/04/01 20:51:55 INFO Server: Started @3196ms\n",
    "19/04/01 20:51:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "19/04/01 20:51:55 INFO AbstractConnector: Started ServerConnector@1bd85e34{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}\n",
    "19/04/01 20:51:55 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2\n",
    "hdfs dfs -cat trianglecount2/* > big_triangle_list.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3\n",
    "scp shuoranl@flux-hadoop-login.arc-ts.umich.edu:~/507/big_triangle_list.txt ./"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
